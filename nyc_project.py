# -*- coding: utf-8 -*-
"""NYC_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w3bi0VpCWIV_IGaB91QfpL8Y9wZ5fqGh

# 1. Machine Learning Part :

# Import all libraries :
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
import math

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor

from sklearn.metrics import r2_score

"""## Preprocessing the data :"""

def preprocess_data(Dataset):

  df = Dataset

  # Shape of train dataset:
  df.shape

  # How dataset looks like:
  df.head()

  # datatypes of cols:
  print("Before Feature Extraction :")
  df.info()

  # find missing value:
  df.isnull().sum()

  # relation between cols:
  df.corr(numeric_only=True)

  # Check duplicated values or not:
  df.duplicated().sum()

  def check_weekend(day):
    return int((day==6) or (day==7))

  def rush_hour(hour):
    return int((hour>=7 and hour<=10) | (hour>=17 and hour<=21))

  # Convert into date_time object because we want to extract something from datetime col
  df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])

  # Find day of week
  df["day_name"] = df['pickup_datetime'].dt.day_name()
  df['day_of_week'] = df['pickup_datetime'].dt.day_of_week

  # Find Weekend or not
  df['Weekend'] = df['day_of_week'].map(check_weekend)

  # Find day of hours
  df['day_of_hours'] = df['pickup_datetime'].dt.hour
  df['rush_hour'] = df['day_of_hours'].map(rush_hour)

  # Find date
  df['pickup_date'] = df['pickup_datetime'].dt.date

  # Convert Co-ordinates(degree) to radians
  def convert_into_radian(degree):
    return ((degree*(math.pi))/(180))

  # Apply Conversion in radians method on Co-ordinates cols
  df['pickup_longitude_rad'] = df['pickup_longitude'].map(convert_into_radian)
  df['pickup_latitude_rad'] = df['pickup_latitude'].map(convert_into_radian)
  df['dropoff_longitude_rad'] = df['dropoff_longitude'].map(convert_into_radian)
  df['dropoff_latitude_rad'] = df['dropoff_latitude'].map(convert_into_radian)

  # Create a method to calculate trip distance
  def distance(pick_lon,pick_lat,drop_lon,drop_lat):
    a = ((math.sin((drop_lat-pick_lat)/2))**2) + math.cos(pick_lat)*math.cos(drop_lat)*((math.sin((drop_lon-pick_lon)/2))**2)
    c = 2 * math.atan2(math.sqrt(a),math.sqrt(1-a))
    R = 6371     # Radius of Earth in KM
    distance = R*c
    return distance

  # Calculate trip distance
  df['trip_distance(KM)'] = list(
      map(distance,
          df['pickup_longitude_rad'],
          df['pickup_latitude_rad'],
          df['dropoff_longitude_rad'],
          df['dropoff_latitude_rad'])
  )

  df['store_and_fwd_flag'] = df['store_and_fwd_flag'].map({'Y':1,'N':0})

  # Calculate daily trip counts and It contains in Each row as total trips on that day
  daily_trips = df.groupby('pickup_date').size().reset_index(name='trip_count')

  # Calculate mean and standard deviation
  mean_trips = daily_trips['trip_count'].mean()
  std_trips = daily_trips['trip_count'].std()

  # this is my threshold I checked 1,2,3 and choose 1.5
  threshold = 1.5

  # Below lower_bound -> unusually low traffic
  # Above upper_bound -> unusually high traffic
  lower_bound = mean_trips - threshold * std_trips
  upper_bound = mean_trips + threshold * std_trips

  # High anomaly -> festivals, strikes ending, storms, events
  # Low anomaly -> lockdowns, extreme weather, holidays
  high_anomaly = daily_trips[daily_trips['trip_count'] > upper_bound]
  low_anomaly = daily_trips[daily_trips['trip_count'] < lower_bound]

  # Combine high and low anomalies
  anomalous_days = pd.concat([high_anomaly, low_anomaly])

  # Normal day as '0'
  df['is_anomaly'] = 0

  # anomalous day as '1'
  df.loc[df['pickup_date'].isin(anomalous_days['pickup_date']), 'is_anomaly'] = 1

  # Calculate speed (distance in km / duration in hours)
  if 'trip_duration' in df.columns:
    df = df[df['trip_duration'] > 0].copy()    # protect against inf
    df['speed'] = df['trip_distance(KM)'] / (df['trip_duration'] / 3600)
  else:
    # Dummy speed for test data (not used further)
    df['speed'] = 0.0

  # Calculate average speed for each day of week for each hour
  df_avg = df.groupby(['day_of_week', 'day_of_hours'])['speed'].mean().reset_index()

  # # Visualize to make decision for traffic
  fig, ax = plt.subplots(figsize=(10, 10))
  df_temp = df_avg.pivot(index='day_of_hours', columns='day_of_week', values='speed')
  sns.heatmap(df_temp, annot=True, ax=ax)
  plt.show()

  # We create a column for traffic is more or less
  # Initially,we let there is no traffic,it means all value of col contain 0
  df['is_traffic'] = 0

  # On the basic of avg speed of every day for each hour(Above heatmap),we decide this
  df.loc[
      (((df['day_of_hours']>=8) & (df['day_of_hours'] <= 18)) &
       ((df['day_of_week']>=1) & (df['day_of_week'] <= 4))),
      'is_traffic'
  ] = 1

  # Check unique values and their counts
  passenger_counts = df['passenger_count'].value_counts().sort_index()

  # I decided to remove 7, 8, 9 based on the counts very small, and I keep 0 because it could be realistic scenario
  # Filter for common values and decide outliers
  outliers = [7, 8, 9]
  df = df[~df['passenger_count'].isin(outliers)].copy()

  # Trips starting/ending inside NYC , Detect outliers / bad GPS points , Improve fare prediction , trip duration models &Filter unrealistic trips

  # Any GPS point inside this box is considered inside boundary.
  manhattan_bounds = {
      'lat_min': 40.70,
      'lat_max': 40.88,
      'lon_min': -74.02,
      'lon_max': -73.90
  }

  df['pickup_in_nyc'] = (
      (df['pickup_latitude'].between(40.70, 40.88)) &
      (df['pickup_longitude'].between(-74.02, -73.90))
  )

  df['dropoff_in_nyc'] = (
      (df['dropoff_latitude'].between(40.70, 40.88)) &
      (df['dropoff_longitude'].between(-74.02, -73.90))
  )

  # NYC Airports (lat, lon): JFK, LaGuardia, Newark
  airports = np.array([
      [40.6413, -73.7781],  # JFK
      [40.7769, -73.8740],  # LaGuardia
      [40.6895, -74.1745]   # Newark
  ])

  airport_lats = np.radians(airports[:, 0])
  airport_lons = np.radians(airports[:, 1])

  R = 6371.0      # Earth radius in km
  radius = 1.5    # km threshold

  # Pickup distance
  pickup_lat = np.radians(df['pickup_latitude'].values.reshape(-1, 1))
  pickup_lon = np.radians(df['pickup_longitude'].values.reshape(-1, 1))

  dlat_p = airport_lats.reshape(1, -1) - pickup_lat
  dlon_p = airport_lons.reshape(1, -1) - pickup_lon

  a_p = np.sin(dlat_p / 2)**2 + \
        np.cos(pickup_lat) * np.cos(airport_lats.reshape(1, -1)) * np.sin(dlon_p / 2)**2

  c_p = 2 * np.arctan2(np.sqrt(a_p), np.sqrt(1 - a_p))
  dist_pickup = R * c_p

  df['pickup_near_airport'] = (dist_pickup <= radius).any(axis=1).astype(int)

  # Dropoff distance
  drop_lat = np.radians(df['dropoff_latitude'].values.reshape(-1, 1))
  drop_lon = np.radians(df['dropoff_longitude'].values.reshape(-1, 1))

  dlat_d = airport_lats.reshape(1, -1) - drop_lat
  dlon_d = airport_lons.reshape(1, -1) - drop_lon

  a_d = np.sin(dlat_d / 2)**2 + \
        np.cos(drop_lat) * np.cos(airport_lats.reshape(1, -1)) * np.sin(dlon_d / 2)**2

  c_d = 2 * np.arctan2(np.sqrt(a_d), np.sqrt(1 - a_d))
  dist_dropoff = R * c_d

  df['dropoff_near_airport'] = (dist_dropoff <= radius).any(axis=1).astype(int)

  # Combine pickup & dropoff coordinates
  coords = np.vstack((
      df[['pickup_latitude','pickup_longitude']].values,
      df[['dropoff_latitude','dropoff_longitude']].values
  ))

  # Fit KMeans
  kmeans = KMeans(n_clusters=20, random_state=42)
  kmeans.fit(coords)

  # Predict clusters (USE .values)
  df['pickup_cluster'] = kmeans.predict(
      df[['pickup_latitude','pickup_longitude']].values
  )

  df['dropoff_cluster'] = kmeans.predict(
      df[['dropoff_latitude','dropoff_longitude']].values
  )

  # Cluster + hour feature
  df['cluster_hour'] = (
      df['pickup_cluster'].astype(str) + "_" +
      df['day_of_hours'].astype(str)
  )

  # Mean trip duration per cluster-hour
  df['avg_cluster_duration'] = (
      df.groupby('cluster_hour')['trip_duration']
              .transform('mean')
  )

  # Convert trip_duration as normally distributed col
  df['trip_duration_log'] = np.log1p(df['trip_duration'])

  # Drop the column
  df = df.drop(
    columns=[
    'id','pickup_datetime','dropoff_datetime','pickup_date','day_name','pickup_longitude_rad','pickup_latitude_rad','dropoff_longitude_rad','dropoff_latitude_rad','speed',
    'cluster_hour','trip_duration'
    ],
    errors="ignore"
  )

  print("After Feature Extraction :")
  print(df.info())

  X = df.drop(columns=['trip_duration_log'])
  y = df['trip_duration_log']

  std=StandardScaler()
  X_transformed=std.fit_transform(X)

  return X_transformed, y

"""# Load the data :"""

df_train=pd.read_csv('train.csv')

# Shape of train dataset:
df_train.shape

df_train.columns

df_val=pd.read_csv("val.csv")

df_val.shape

df_val.columns

df_train_X,df_train_y=preprocess_data(df_train)
df_val_X,df_val_y=preprocess_data(df_val)

df_train_X

df_train_y

df_val_X

df_val_y

"""# Linear Regression :"""

lr=LinearRegression()
lr.fit(df_train_X,df_train_y)
y_pred_lr=lr.predict(df_val_X)
r2_score(df_val_y,y_pred_lr)

"""# Decision Tree Regressor :"""

# Initialize model
dt = DecisionTreeRegressor(
    max_depth=12,          # controls overfitting
    min_samples_split=50,  # minimum samples to split
    min_samples_leaf=25,   # minimum samples at leaf
    random_state=42
)

# Train
dt.fit(df_train_X, df_train_y)

# Predict
y_pred_dt = dt.predict(df_val_X)

# RÂ² score
r2_dt = r2_score(df_val_y, y_pred_dt)
print("Decision Tree R2 Score:", r2_dt)

"""# Random Forest Regressor :"""

rf = RandomForestRegressor(
    n_estimators=150,        # number of trees
    max_depth=18,            # control tree size
    min_samples_split=50,    # avoid overfitting
    min_samples_leaf=25,
    max_features="sqrt",     # feature subsampling
    n_jobs=-1,               # use all cores
    random_state=42,
    verbose=1
)

# Train
rf.fit(df_train_X, df_train_y)

# Predict
y_pred_rf = rf.predict(df_val_X)

# R2 score
r2_rf = r2_score(df_val_y, y_pred_rf)
print("Random Forest R2 Score:", r2_rf)

"""# 2. Neural Network Part :"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Create Neural Network Architecture :
model = Sequential([
    Dense(256, activation='relu', input_shape=(df_train_X.shape[1],)),
    BatchNormalization(),
    Dropout(0.5),

    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),

    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.25),

    Dense(1)   # Regression output
])

# comiple the model
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='mse',
    metrics=['mae']
)

# Import Early Stopping
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=15,
    restore_best_weights=True
)

# Fit the model
history = model.fit(
    df_train_X,
    df_train_y,
    validation_data=(df_val_X, df_val_y),
    epochs=100,
    batch_size=512,   # large batch works well here
    callbacks=[early_stop],
    verbose=1
)

# Find R2_Score
from sklearn.metrics import r2_score
import numpy as np

y_pred_nn = model.predict(df_val_X).ravel()
r2_nn = r2_score(df_val_y, y_pred_nn)

print("Neural Network R2 Score:", r2_nn)

# Plot the graph
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')

plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.title('Training vs Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

